import argparse
import os
import sys
import copy

import ujson as json  # ä½¿ç”¨æ›´å¿«çš„ ujson åº“å¤„ç† JSON

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

from copypastelrm.metrics.HotpotQA import update_answer, update_sp

from copypastelrm.metrics.utils import extract_answer_and_facts, extract_answer_and_facts_old

from utils.git import get_git_commit_id


def log_result(result: dict):
    """
    æ ¼å¼åŒ–è¾“å‡ºç»“æœä¸ºObsidian Yamlæ ¼å¼ï¼Œæ–¹ä¾¿è®°å½•
    """
    print("metrics:")
    for key, value in result.items():
        print(f"\t- {key}={value}")


def eval(path: str):
    """
    ä¸»è¯„ä¼°å‡½æ•°ï¼Œæ‰§è¡Œå®Œæ•´çš„ HotpotQA è¯„ä¼°æµç¨‹

    è¯„ä¼°æµç¨‹ï¼š
    1. åŠ è½½é¢„æµ‹æ–‡ä»¶
    2. ä» HuggingFace åŠ è½½æ ‡å‡†ç­”æ¡ˆæ•°æ®é›†
    3. éå†æ¯ä¸ªæ ·æœ¬ï¼Œè®¡ç®—ç­”æ¡ˆå’Œæ”¯æŒäº‹å®æŒ‡æ ‡
    4. è®¡ç®—è”åˆæŒ‡æ ‡ï¼ˆç­”æ¡ˆå’Œæ”¯æŒäº‹å®çš„ä¹˜ç§¯ï¼‰
    5. è¾“å‡ºå¹³å‡æŒ‡æ ‡

    Args:
        prediction_file (str): é¢„æµ‹ç»“æœæ–‡ä»¶è·¯å¾„
    """
    # åŠ è½½é¢„æµ‹ç»“æœæ–‡ä»¶
    with open(path) as f:
        prediction = json.load(f)
    
    if 'info' not in prediction:
        print('ğŸ˜¬ å°šæœªæ¨ç†å®Œæˆï¼Œè·³è¿‡:', path)
        return None

    info = prediction.get("info")
    dataset_name = info.get("dataset")

    data = prediction.get("data")

    # åˆå§‹åŒ–æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡
    metrics = {
        # ç­”æ¡ˆç›¸å…³æŒ‡æ ‡
        "hit": 0,
        "em": 0,  # ç²¾ç¡®åŒ¹é…
        "f1": 0,  # F1 åˆ†æ•°
        "prec": 0,  # ç²¾ç¡®ç‡
        "recall": 0,  # å¬å›ç‡
        # æ”¯æŒäº‹å®ç›¸å…³æŒ‡æ ‡
        "sp_em": 0,  # æ”¯æŒäº‹å®ç²¾ç¡®åŒ¹é…
        "sp_f1": 0,  # æ”¯æŒäº‹å® F1 åˆ†æ•°
        "sp_prec": 0,  # æ”¯æŒäº‹å®ç²¾ç¡®ç‡
        "sp_recall": 0,  # æ”¯æŒäº‹å®å¬å›ç‡
        # è”åˆæŒ‡æ ‡
        "joint_em": 0,  # è”åˆç²¾ç¡®åŒ¹é…
        "joint_f1": 0,  # è”åˆ F1 åˆ†æ•°
        "joint_prec": 0,  # è”åˆç²¾ç¡®ç‡
        "joint_recall": 0,  # è”åˆå¬å›ç‡
        # count
        "count": 0,
        "without_answer_ids": [],
        "without_facts_ids": [],
        "context_length": 0,
        "context_length_word": 0,
        "predict_length": 0,
        "predict_length_word": 0,
        "answer_length": 0,
        "answer_length_word": 0
    }

    metrics_by_subset = {}

    # éå†æ¯ä¸ªæ ·æœ¬è¿›è¡Œè¯„ä¼°
    for id, item in data.items():
        can_eval_joint = True  # æ ‡è®°æ˜¯å¦å¯ä»¥è®¡ç®—è”åˆæŒ‡æ ‡
        prompt_type = info.get("prompt_type")

        subset = item.get("dataset")

        if subset not in metrics_by_subset:
            metrics_by_subset[subset] = copy.deepcopy(metrics)

        metrics_by_subset[subset]["count"] += 1

        if item['context']:
            metrics_by_subset[subset]['context_length'] += len(item['context'])
            metrics_by_subset[subset]['context_length_word'] += len(item['context'].split())

        if item['predict']:
            metrics_by_subset[subset]['predict_length'] += len(item['predict'])
            metrics_by_subset[subset]['predict_length_word'] += len(item['predict'].split())

        if "old" in prompt_type:
            predicted_answer, predicted_facts = extract_answer_and_facts_old(item["predict"])
        else:
            predicted_answer, predicted_facts = extract_answer_and_facts(item["predict"])

        if predicted_answer:
            metrics_by_subset[subset]['answer_length'] += len(predicted_answer)
            metrics_by_subset[subset]['answer_length_word'] += len(predicted_answer.split())

        gold_answers = None
        if isinstance(item["answer"], list):
            if len(item["answer"]) == 0:
                predicted_answer = None
            else:
                gold_answers = item["answer"]
        elif isinstance(item["answer"], str):
            gold_answers = [item["answer"]]

        # è¯„ä¼°ç­”æ¡ˆéƒ¨åˆ†
        if predicted_answer is None:
            metrics_by_subset[subset]["without_answer_ids"].append(id)
            can_eval_joint = False
        else:
            # è®¡ç®—ç­”æ¡ˆæŒ‡æ ‡å¹¶æ›´æ–°
            em, prec, recall = update_answer(
                metrics_by_subset[subset], predicted_answer, gold_answers
            )

        # è¯„ä¼°æ”¯æŒäº‹å®éƒ¨åˆ†
        if len(predicted_facts) == 0:
            metrics_by_subset[subset]["without_facts_ids"].append(id)
            can_eval_joint = False
        else:
            gold_supporting_facts = item["sfs"]
            # è®¡ç®—æ”¯æŒäº‹å®æŒ‡æ ‡å¹¶æ›´æ–°
            sp_em, sp_prec, sp_recall, _ = update_sp(
                metrics_by_subset[subset], predicted_facts, gold_supporting_facts
            )

        # è®¡ç®—è”åˆæŒ‡æ ‡ï¼ˆåªæœ‰å½“ç­”æ¡ˆå’Œæ”¯æŒäº‹å®éƒ½å­˜åœ¨æ—¶æ‰è®¡ç®—ï¼‰
        if can_eval_joint:
            # è”åˆç²¾ç¡®ç‡ = ç­”æ¡ˆç²¾ç¡®ç‡ * æ”¯æŒäº‹å®ç²¾ç¡®ç‡
            joint_prec = prec * sp_prec
            # è”åˆå¬å›ç‡ = ç­”æ¡ˆå¬å›ç‡ * æ”¯æŒäº‹å®å¬å›ç‡
            joint_recall = recall * sp_recall
            # è®¡ç®—è”åˆ F1 åˆ†æ•°
            if joint_prec + joint_recall > 0:
                joint_f1 = 2 * joint_prec * joint_recall / (joint_prec + joint_recall)
            else:
                joint_f1 = 0.0
            # è”åˆç²¾ç¡®åŒ¹é… = ç­”æ¡ˆç²¾ç¡®åŒ¹é… * æ”¯æŒäº‹å®ç²¾ç¡®åŒ¹é…
            joint_em = em * sp_em

            # ç´¯åŠ è”åˆæŒ‡æ ‡
            metrics_by_subset[subset]["joint_em"] += joint_em
            metrics_by_subset[subset]["joint_f1"] += joint_f1
            metrics_by_subset[subset]["joint_prec"] += joint_prec
            metrics_by_subset[subset]["joint_recall"] += joint_recall

    # ä½¿ç”¨é¢„æµ‹æ•°é‡ä½œä¸ºåˆ†æ¯è®¡ç®—å¹³å‡æŒ‡æ ‡
    for subset, subset_metrics in metrics_by_subset.items():
        prediction_count = subset_metrics["count"]
        for k in subset_metrics.keys():
            if k != "without_answer_ids" and k != "without_facts_ids" and k != "count":
                subset_metrics[k] /= prediction_count
        
        without_answer_ids_set = set(subset_metrics["without_answer_ids"])
        without_facts_ids_set = set(subset_metrics["without_facts_ids"])
        subset_metrics["without answer only samples"] = len(
            without_answer_ids_set - without_facts_ids_set
        )
        subset_metrics["without facts only samples"] = len(
            without_facts_ids_set - without_answer_ids_set
        )
        subset_metrics["without answer and facts samples"] = len(
            without_answer_ids_set & without_facts_ids_set
        )
        subset_metrics["with answer and facts samples"] = prediction_count - len(
            without_answer_ids_set | without_facts_ids_set
        )

        subset_metrics["without_answer_ids"] = len(subset_metrics["without_answer_ids"])
        subset_metrics["without_facts_ids"] = len(subset_metrics["without_facts_ids"])

    res = {
        "project": "CopyPasteLRM",
        "type": "Experiment",
        "method": info["model_name"],
        "dataset": dataset_name,
        "eval git commit id": get_git_commit_id(),
        "infer git commit id": info.get("infer_git_commit_id"),
        "infer start time": info.get("start_time"),
        "infer end time": info.get("end_time"),
        "server url": info.get("server_url"),
        "prompt type": info.get("prompt_type"),
        "prompt snapshot": info.get("prompt_snapshot"),
        "temperature": info.get("temperature"),
        "top p": info.get("top_p"),
        "metrics": metrics_by_subset,
        "total samples": len(data),
        "output file": path,
    }


    output_file = path.replace("results/infer", "results/eval")
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    with open(output_file, "w", encoding="utf-8") as f:
        json.dump(res, f, ensure_ascii=False, indent=2)
    print(f"âœ… Eval results saved to {output_file}")

    # è¾“å‡ºæœ€ç»ˆè¯„ä¼°ç»“æœ
    # log_result(metrics)
    print(json.dumps(res, ensure_ascii=False, indent=2))


def main():
    parser = argparse.ArgumentParser(description="Evaluation Script")
    parser.add_argument("path")
    args = parser.parse_args()

    eval(args.path)
    # eval('results/infer/resamples_-1/seed_42/tpr_0.7-tpp_0.95/Qwen2.5-3B-Instruct/copypaste/enable_thinking_False-prompt_reasoning_with_copypaste_old-1765207378.json')

if __name__ == "__main__":
    main()
