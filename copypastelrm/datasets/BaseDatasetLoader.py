from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional
from datasets import load_dataset
from tqdm import tqdm
import os
import json
import random
from copypastelrm.utils.tokenizer import ChatTokenCounter
from copypastelrm.utils.bm25 import BM25Retriever
from copypastelrm.utils.dataset import NLPTool


class BaseDatasetLoader(ABC):
    """
    æ•°æ®é›†åŠ è½½å™¨çš„åŸºç±»ï¼Œæä¾›ç»Ÿä¸€çš„æ•°æ®é›†åŠ è½½å’Œé¢„å¤„ç†æ¥å£
    """

    def __init__(
        self,
        dataset_path: str,
        split: str,
        cache_dir: str = "data/cache/",  # ç¼“å­˜è·¯å¾„
        dataset_name: Optional[str] = None,
        offline: bool = True,
        reload: bool = False,
        format: bool = True,
        max_samples: int = -1,
        filter_empty_answer: bool = True,
        distractor_docs: int = 8,
        unanswerable: bool = False, # æ˜¯å¦ä¸åŒ…å«gold context
        # max_input_tokens: int = 1024 * 24,
        # tokenizer_path: str = 'Qwen/Qwen2.5-3B-Instruct',
    ):
        """
        åˆå§‹åŒ–æ•°æ®é›†åŠ è½½å™¨

        Args:
            dataset_path: HuggingFace æ•°æ®é›†è·¯å¾„
            dataset_name: æ•°æ®é›†å­é›†åç§°ï¼ˆå¯é€‰ï¼‰
            split: æ•°æ®é›†åˆ†å‰²ï¼ˆé»˜è®¤ä¸º validationï¼‰
            offline: æ˜¯å¦ç¦»çº¿æ¨¡å¼
        """
        self.nlp = NLPTool()
        self.dataset_path = dataset_path
        self.dataset_name = dataset_name
        self.split = split
        self.offline = offline
        self.reload = reload
        self.format = format  # æ˜¯å¦æ ¼å¼åŒ–æ•°æ®é›†
        if self.dataset_name:
            file_name = f"{self.dataset_path.replace('.json', '').replace('/', '-')}-{self.dataset_name}-{self.split}"
        else:
            file_name = f"{self.dataset_path.replace('.json', '').replace('/', '-')}-{self.split}"
        self.cache_path = cache_dir + f"{file_name}.jsonl"
        self.filter_empty_answer = filter_empty_answer

        # æ£€æŸ¥cache_pathæ˜¯å¦ä»¥.jsonlç»“å°¾
        if self.cache_path:
            if not self.cache_path.endswith(".jsonl"):
                raise ValueError("cache_path must end with .jsonl")

        self.unanswerable = unanswerable

        self.dataset_list = None
        self.distractor_docs = distractor_docs

        self.origin_dataset = self.get_dataset()
        self.corpus = self.construct_corpus(self.origin_dataset)
        self.bm25 = BM25Retriever(self.corpus)
        self.dataset = self.format_dataset(self.origin_dataset)

        if max_samples > 0 and max_samples < self.get_length():
            self.dataset_list = random.sample(self.dataset_list, max_samples)
            self.dataset_dict = {}
            for sample in self.dataset_list:
                self.dataset_dict[sample["id"]] = sample
            self.dataset = self.dataset_dict

        assert len(self.dataset_list) == len(self.dataset), "æ•°æ®é›†åˆ—è¡¨å’Œå­—å…¸é•¿åº¦ä¸ä¸€è‡´"

    def download_dataset(self) -> List[Dict[str, Any]]:
        """é»˜è®¤ä»huggingfaceä¸‹è½½æ•°æ®"""
        print(f"æ­£åœ¨åŠ è½½ {self.dataset_path} æ•°æ®é›†...")
        if self.dataset_name:
            print(f"æ•°æ®é›†å­é›†: {self.dataset_name}")
        print(f"æ•°æ®åˆ†å‰²: {self.split}")

        if self.dataset_name:
            dataset = load_dataset(
                path=self.dataset_path, name=self.dataset_name, split=self.split
            )
        else:
            dataset = load_dataset(path=self.dataset_path, split=self.split)

        dataset = list(dataset)

        return dataset

    def get_dataset(self):
        """
        åŠ è½½æ•°æ®é›†

        Args:
            origin: æ˜¯å¦è¿”å›åŸå§‹æ•°æ®é›†ï¼ˆä¸è¿›è¡Œæ ¼å¼åŒ–ï¼‰

        Returns:
            List[Dict]: åŒ…å«æ•°æ®æ ·æœ¬çš„åˆ—è¡¨
        """

        if (
            not self.reload
            and self.offline
            and self.cache_path
            and os.path.exists(self.cache_path)
        ):
            with open(self.cache_path, "r") as f:
                print(f"ğŸ¯Loading dataset from cache: {self.cache_path}")
                formatted_dataset_list = json.load(f)
                if self.filter_empty_answer:
                    formatted_dataset_list = self.get_non_empty_answer(formatted_dataset_list)
                    random.shuffle(formatted_dataset_list)
                self.dataset_list = formatted_dataset_list
                dataset_dict = {}
                for sample in formatted_dataset_list:
                    dataset_dict[sample["id"]] = sample
                return dataset_dict

        origin_dataset = self.download_dataset()
        return origin_dataset
    
    def format_dataset(self, origin_dataset: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ ¼å¼åŒ–æ•°æ®é›†

        Args:
            dataset: åŸå§‹æ•°æ®é›†

        Returns:
            Dict[str, Any]: æ ¼å¼åŒ–åçš„æ•°æ®é›†
        """

        formatted_dataset_dict = {}
        formatted_dataset_list = []

        iterator = tqdm(origin_dataset, desc="Formatting dataset", unit="sample")

        for sample in iterator:
            if self.format:
                formatted_sample = self.format_sample(sample)
            else:
                formatted_sample = sample
            formatted_dataset_list.append(formatted_sample)
            formatted_dataset_dict[formatted_sample["id"]] = formatted_sample

        # å¦‚æœå¼€å¯ç¦»çº¿æ¨¡å¼å¹¶ä¸”æŒ‡å®šäº†ç¼“å­˜è·¯å¾„ï¼Œåˆ™å°†æ ¼å¼åŒ–åçš„æ•°æ®é›†ä¿å­˜åˆ°ç¼“å­˜æ–‡ä»¶
        if self.offline and self.cache_path:
            os.makedirs(os.path.dirname(self.cache_path), exist_ok=True)
            with open(self.cache_path, "w") as f:
                print(f"Saving formatted dataset to cache: {self.cache_path}")
                json.dump(
                    list(formatted_dataset_dict.values()),
                    f,
                    ensure_ascii=False,
                    indent=4,
                )

        if self.filter_empty_answer:
            formatted_dataset_list = self.get_non_empty_answer(formatted_dataset_list)
            random.shuffle(formatted_dataset_list)
            formatted_dataset_dict = {}
            for sample in formatted_dataset_list:
                formatted_dataset_dict[sample["id"]] = sample
        self.dataset_list = formatted_dataset_list

        return formatted_dataset_dict
    
    def get_non_empty_answer(self, data: list) -> list:
        return [sample for sample in data if len(sample["answers"]) > 0 and sample["answers"][0].strip() != ""]

    def format_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ¼å¼åŒ–å•ä¸ªæ•°æ®æ ·æœ¬

        Args:
            sample: åŸå§‹æ•°æ®æ ·æœ¬

        Returns:
            Dict[str, Any]: æ ¼å¼åŒ–åçš„æ•°æ®æ ·æœ¬
        """
        item = self.format_item(sample)
        context, facts = self.construct_context_and_facts(item)

        formatted_sample = {
            "id": item['id'],
            "query": item['query'],
            "answers": item['answers'],
            "context": "\n\n".join(context),
            "facts": facts,
            "corpus": item['corpus'],
            "extra": item['extra'],
            "dataset": self.dataset_path if 'dataset' not in sample else sample['dataset'],
        }

        return formatted_sample


    # å­ç±»å¿…é¡»å®ç°çš„å‡½æ•°: format_corpus
    @abstractmethod
    def format_item(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ ¼å¼åŒ–è¯­æ–™åº“ï¼Œå¿…é¡»è¿”å›æ•°æ®ç»“æ„

        {
            "id": str,
            "query": str,
            "answers": List[str],
            "corpus": [
                {
                    "title": Optinal[str, None],
                    "sentences": List[str], # å¿…é¡»ç»è¿‡åˆ†å¥
                    "facts": Optional[List[str], None], # å¦‚æœæ²¡æœ‰ï¼Œåˆ™è¿”å› None
                }
            ],
            "extra": Optional[Dict[str, Any], None]
        }

        Args:
            sample: åŸå§‹æ•°æ®æ ·æœ¬

        Returns:
            str: ä¸Šä¸‹æ–‡æ–‡æœ¬
        """
        raise NotImplementedError("å­ç±»å¿…é¡»å®ç° format_corpus æ–¹æ³•")

    @staticmethod
    def string_context(title: str, sentences: List[str]) -> str:
        return "###" + title.upper() + "\n" + " ".join(sentences)
    
    @staticmethod
    def string_sub_id(id: str, idx: int) -> str:
        return id + "___" + str(idx)

    def construct_corpus(self, data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        global_corpus = []
        for sample in data:
            formated_item = self.format_item(sample) 
            _id = formated_item["id"]
            corpus = formated_item['corpus']
            for idx, context in enumerate(corpus):
                text = self.string_context(context['title'], context['sentences'])
                global_corpus.append({
                    "id": self.string_sub_id(_id, idx),
                    "text": text,
                })
        
        # é’ˆå¯¹textè¿›è¡Œå»é‡ï¼Œä¿ç•™corpusçš„ç»“æ„ï¼Œä½†æ˜¯å¦‚æœtextçš„å†…å®¹é‡å¤äº†ï¼Œåˆ™ä¿ç•™ç¬¬ä¸€ä¸ªï¼Œåé¢çš„å‡å‰”é™¤
        global_corpus = list({item['text']: item for item in global_corpus}.values())

        return global_corpus

    
    def construct_context_and_facts(self, format_item: Dict[str, Any]) -> tuple[str, str, str, str, List[str]]:
        """
        Args:
            context: åŒ…å« title å’Œ sentences çš„å­—å…¸

        Returns:

        """

        _id = format_item["id"]
        query = format_item['query']
        single_corpus = format_item['corpus']

        gold_context = []
        gold_ctx_ids = []
        facts = []
        for idx, item in enumerate(single_corpus):
            if item['facts']:
                gold_context.append(self.string_context(item['title'], item['sentences']))
                gold_ctx_ids.append(self.string_sub_id(_id, idx))
                facts.extend(item['facts'])
        
        distractor_context = []
        
        if self.distractor_docs > 0:
            candidate_distractor_context = self.bm25.retrieve(query, k=self.distractor_docs + len(gold_context) + 10)

            distractor_count = 0

            for item in candidate_distractor_context:
                if item['id'] not in gold_ctx_ids and item['text'] not in gold_context:
                    distractor_context.append(item['text'])
                    distractor_count += 1
                    if distractor_count >= self.distractor_docs:
                        break
        
        # åˆå¹¶ gold context å’Œ distractor contextï¼Œå¹¶ä¸”æ‰“ä¹±
        if self.unanswerable:
            context = distractor_context
            facts = []
        else:
            context = gold_context + distractor_context

        random.seed(42)
        random.shuffle(context)

        return context, facts


    def get_length(self) -> int:
        """
        è·å–æ•°æ®é›†æ ·æœ¬æ•°é‡

        Returns:
            int: æ•°æ®é›†æ ·æœ¬æ•°é‡
        """
        return len(self.dataset)

    def get_sample(self, sample_id=None) -> Dict[str, Any]:
        """
        æ ¹æ®æ ·æœ¬ ID è·å–æ ·æœ¬

        Args:
            sample_id: æ ·æœ¬ ID

        Returns:
            Dict[str, Any]: æ ·æœ¬æ•°æ®
        """
        if sample_id:
            return self.dataset[sample_id]
        else:
            sample_id = random.choice(list(self.dataset.keys()))
        return self.dataset.get(sample_id, None)

    def random_sample(self) -> Dict[str, Any]:
        """
        éšæœºè·å–ä¸€ä¸ªæ ·æœ¬

        Returns:
            Dict[str, Any]: éšæœºæ ·æœ¬
        """
        sample_id = random.choice(list(self.dataset.keys()))
        sample = self.dataset[sample_id]
        print(f"ID: {sample_id}")
        print("-" * 20)
        print(f"Query: {sample['query']}")
        print("-" * 20)
        print(f"Context: {sample['context']}")
        print("-" * 20)
        print(f"Answers: {sample['answers']}")
        print("-" * 20)
        if "facts" in sample:
            print(f"Supporting Facts: {sample['facts']}")
