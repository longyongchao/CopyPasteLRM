import gzip  # <--- æ–°å¢žå¼•å…¥
import os
import json
import random
from abc import ABC, abstractmethod
from typing import Any, Dict, List, Optional, Tuple
from datasets import load_dataset
from tqdm import tqdm
from copypastelrm.utils.dataset import NLPTool
from copypastelrm.utils.bm25 import BM25Retriever 

class BaseDatasetLoader(ABC):
    def __init__(
        self,
        dataset_path: str,
        split: str,
        cache_dir: str = "/tmp/copypastelrm/cache/",
        dataset_name: Optional[str] = None,
        offline: bool = True,
        reload: bool = False,
        format: bool = True,
        max_samples: int = -1,
        filter_empty_answer: bool = True,
        distractor_docs: int = 8,
        unanswerable: bool = False, 
    ):
        self.nlp = NLPTool()
        self.dataset_path = dataset_path
        self.dataset_name = dataset_name
        self.split = split
        self.offline = offline
        self.reload = reload
        self.format = format
        self.filter_empty_answer = filter_empty_answer
        self.unanswerable = unanswerable
        self.distractor_docs = distractor_docs
        
        # -----------------------------------------------------------
        # ä¿®æ”¹ç‚¹ 1: ç¼“å­˜æ–‡ä»¶ååŽç¼€æ”¹ä¸º .jsonl.gz
        # -----------------------------------------------------------
        base_name = self.dataset_path.replace('.json', '').replace('/', '-')
        subset_name = f"-{self.dataset_name}" if self.dataset_name else ""
        file_name = f"{base_name}{subset_name}-{self.split}-noise_{self.distractor_docs}-{'unanswerable' if self.unanswerable else 'answerable'}"

        # æ”¹ç”¨ .jsonl.gz
        self.cache_path = os.path.join(cache_dir, f"{file_name}.jsonl.gz")

        # -----------------------------------------------------------
        # Step 1: åŠ è½½æ•°æ®
        # -----------------------------------------------------------
        self.dataset_list, self.is_from_cache = self.get_dataset()
        
        if not self.dataset_list:
            print("âš ï¸ Warning: Loaded dataset is empty.")

        # -----------------------------------------------------------
        # Step 2: æž„å»ºæ£€ç´¢å™¨
        # -----------------------------------------------------------
        if not self.is_from_cache:
            print('æ­£åœ¨æž„å»º BM25 è¯­æ–™åº“...')
            self.corpus = self.construct_corpus(self.dataset_list, is_formatted=self.is_from_cache)
            if not self.corpus:
                print("âš ï¸ Warning: Corpus is empty. BM25 index will fail.")
            else:
                print(f'è¯­æ–™åº“æž„å»ºå®Œæˆï¼Œå…± {len(self.corpus)} æ¡æ–‡æ¡£ï¼Œå¼€å§‹æž„å»ºç´¢å¼•...')
                self.bm25 = BM25Retriever(self.corpus)

        # -----------------------------------------------------------
        # Step 3: æœ€ç»ˆåŒ–æ•°æ®é›†
        # -----------------------------------------------------------
        if self.is_from_cache:
            print('âœ… æ£€æµ‹åˆ°æ•°æ®æ¥è‡ªç¼“å­˜ (Compressed)ï¼Œè·³è¿‡æ ¼å¼åŒ–æ­¥éª¤ï¼Œç›´æŽ¥åŠ è½½ã€‚')
            self.dataset = {sample["id"]: sample for sample in self.dataset_list}
        else:
            print('ðŸ”„ æ•°æ®ä¸ºåŽŸå§‹æ ¼å¼ï¼Œå¼€å§‹æ‰§è¡Œæ ¼å¼åŒ–ä¸Žæ£€ç´¢...')
            self.dataset = self.format_dataset(self.dataset_list)

        # -----------------------------------------------------------
        # Step 4: é‡‡æ ·
        # -----------------------------------------------------------
        if 0 < max_samples < len(self.dataset_list):
            print(f"Sampling {max_samples} samples from {len(self.dataset_list)} total.")
            self.dataset_list = random.sample(self.dataset_list, max_samples)
            self.dataset = {sample["id"]: sample for sample in self.dataset_list}

        assert len(self.dataset_list) == len(self.dataset), "æ•°æ®é›†åˆ—è¡¨å’Œå­—å…¸é•¿åº¦ä¸ä¸€è‡´"
        print('ðŸŽ‰ æ•°æ®é›†å‡†å¤‡å°±ç»ª')

    # ... download_dataset ä¿æŒä¸å˜ ...
    def download_dataset(self) -> List[Dict[str, Any]]:
        # (ä»£ç çœç•¥ï¼Œä¿æŒåŽŸæ ·)
        print(f"æ­£åœ¨åŠ è½½ {self.dataset_path} æ•°æ®é›†...")
        if self.dataset_name:
            print(f"æ•°æ®é›†å­é›†: {self.dataset_name}")
        print(f"æ•°æ®åˆ†å‰²: {self.split}")

        if self.dataset_name:
            dataset = load_dataset(
                path=self.dataset_path, name=self.dataset_name, split=self.split
            )
        else:
            dataset = load_dataset(path=self.dataset_path, split=self.split)
        
        print(f"æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {len(dataset)} ä¸ªæ ·æœ¬")
        return list(dataset)

    def get_dataset(self) -> Tuple[List[Dict[str, Any]], bool]:
        """
        åŠ è½½æ•°æ®é›† (æ”¯æŒ gzip è¯»å–)ã€‚
        """
        if (
            not self.reload
            and self.offline
            and self.cache_path
            and os.path.exists(self.cache_path)
        ):
            try:
                # -----------------------------------------------------------
                # ä¿®æ”¹ç‚¹ 2: ä½¿ç”¨ gzip.open è¯»å–ï¼Œæ¨¡å¼ä¸º 'rt' (read text)
                # -----------------------------------------------------------
                print(f"ðŸŽ¯ Loading dataset from compressed cache: {self.cache_path}")
                with gzip.open(self.cache_path, "rt", encoding='utf-8') as f:
                    formatted_dataset_list = json.load(f)
                    
                    if self.filter_empty_answer:
                        formatted_dataset_list = self.get_non_empty_answer(formatted_dataset_list)
                        random.shuffle(formatted_dataset_list)
                    
                    return formatted_dataset_list, True
            except Exception as e:
                print(f"âš ï¸ è¯»å–ç¼“å­˜å¤±è´¥: {e}ï¼Œå°†å›žé€€åˆ°ä¸‹è½½æ¨¡å¼ã€‚")

        # æ— ç¼“å­˜æˆ–å¼ºåˆ¶åˆ·æ–°
        origin_dataset = self.download_dataset()
        return origin_dataset, False
    
    def format_dataset(self, origin_dataset: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æ ¼å¼åŒ–æ•°æ®é›†å¹¶ä¿å­˜ (æ”¯æŒ gzip å†™å…¥)ã€‚
        """
        formatted_dataset_dict = {}
        formatted_dataset_list = []

        iterator = tqdm(origin_dataset, desc="Formatting dataset", unit="sample")

        for sample in iterator:
            if self.format:
                formatted_sample = self.format_sample(sample)
            else:
                formatted_sample = sample
            
            formatted_dataset_list.append(formatted_sample)
            formatted_dataset_dict[formatted_sample["id"]] = formatted_sample

        if self.filter_empty_answer:
            formatted_dataset_list = self.get_non_empty_answer(formatted_dataset_list)
            formatted_dataset_dict = {sample["id"]: sample for sample in formatted_dataset_list}
            random.shuffle(formatted_dataset_list)
        
        self.dataset_list = formatted_dataset_list

        if self.offline and self.cache_path:
            os.makedirs(os.path.dirname(self.cache_path), exist_ok=True)
            # -----------------------------------------------------------
            # ä¿®æ”¹ç‚¹ 3: ä½¿ç”¨ gzip.open å†™å…¥ï¼Œæ¨¡å¼ä¸º 'wt' (write text)
            # -----------------------------------------------------------
            print(f"Saving formatted dataset to compressed cache: {self.cache_path}")
            with gzip.open(self.cache_path, "wt", encoding='utf-8') as f:
                json.dump(
                    formatted_dataset_list,
                    f,
                    ensure_ascii=False,
                    indent=4, # å¦‚æžœä¸ºäº†æžè‡´ç©ºé—´ï¼Œå¯ä»¥åŽ»æŽ‰ indent=4ï¼Œå˜æˆç´§å‡‘æ ¼å¼
                )

        return formatted_dataset_dict

    # ... å…¶ä½™å‡½æ•° (get_non_empty_answer, format_sample, format_item, construct_corpus ç­‰) ä¿æŒå®Œå…¨ä¸å˜ ...
    def get_non_empty_answer(self, data: list) -> list:
        return [
            sample for sample in data 
            if "answers" in sample 
            and isinstance(sample["answers"], list) 
            and len(sample["answers"]) > 0 
            and str(sample["answers"][0]).strip() != ""
        ]

    def format_sample(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        item = self.format_item(sample)
        context, facts = self.construct_context_and_facts(item)
        formatted_sample = {
            "id": item['id'],
            "query": item['query'],
            "answers": item['answers'],
            "context": "\n\n".join(context),
            "facts": facts,
            "corpus": item['corpus'],
            "extra": item.get('extra', None),
            "dataset": self.dataset_path if 'dataset' not in sample else sample['dataset'],
        }
        return formatted_sample

    @abstractmethod
    def format_item(self, sample: Dict[str, Any]) -> Dict[str, Any]:
        raise NotImplementedError("å­ç±»å¿…é¡»å®žçŽ° format_item æ–¹æ³•")

    @staticmethod
    def string_context(title: str, sentences: List[str]) -> str:
        return "###" + str(title).upper() + "\n" + " ".join(sentences)
    
    @staticmethod
    def string_sub_id(id: str, idx: int) -> str:
        return f"{id}___{idx}"

    def construct_corpus(self, data: List[Dict[str, Any]], is_formatted: bool = False) -> List[Dict[str, Any]]:
        global_corpus = []
        seen_texts = set()

        for sample in tqdm(data, desc="Constructing corpus"):
            if is_formatted:
                if 'corpus' in sample:
                    _id = sample['id']
                    corpus_items = sample['corpus']
                else:
                    continue
            else:
                formated_item = self.format_item(sample) 
                _id = formated_item["id"]
                corpus_items = formated_item['corpus']
            
            for idx, context in enumerate(corpus_items):
                text = self.string_context(context['title'], context['sentences'])
                if text not in seen_texts:
                    seen_texts.add(text)
                    global_corpus.append({
                        "id": self.string_sub_id(_id, idx),
                        "text": text,
                    })
        return global_corpus

    def construct_context_and_facts(self, format_item: Dict[str, Any]) -> Tuple[List[str], List[str]]:
        _id = format_item["id"]
        query = format_item['query']
        single_corpus = format_item['corpus']
        gold_context = []
        gold_ctx_ids = set()
        facts = []
        for idx, item in enumerate(single_corpus):
            if item.get('facts'):
                ctx_str = self.string_context(item['title'], item['sentences'])
                gold_context.append(ctx_str)
                gold_ctx_ids.add(self.string_sub_id(_id, idx))
                facts.extend(item['facts'])
        
        distractor_context = []
        if self.distractor_docs > 0:
            k_val = self.distractor_docs + len(gold_context) + 10
            candidate_distractor_context = self.bm25.retrieve(query, k=k_val)
            distractor_count = 0
            for item in candidate_distractor_context:
                if item['id'] not in gold_ctx_ids and item['text'] not in gold_context:
                    distractor_context.append(item['text'])
                    distractor_count += 1
                    if distractor_count >= self.distractor_docs:
                        break
        
        if self.unanswerable:
            context = distractor_context
            facts = []
        else:
            context = gold_context + distractor_context

        rng = random.Random(42) 
        rng.shuffle(context)
        return context, facts

    def get_length(self) -> int:
        return len(self.dataset)

    def get_sample(self, sample_id=None) -> Dict[str, Any]:
        if not self.dataset: return None
        if sample_id: return self.dataset.get(sample_id)
        else:
            sample_id = random.choice(list(self.dataset.keys()))
            return self.dataset[sample_id]

    def random_sample(self) -> Dict[str, Any]:
        if not self.dataset:
            print("Dataset is empty.")
            return {}
        sample_id = random.choice(list(self.dataset.keys()))
        sample = self.dataset[sample_id]
        print(f"ID: {sample_id}")
        print("-" * 20)
        print(f"Query: {sample['query']}")
        print("-" * 20)
        print(f"Context (Preview): {sample['context'][:500]}..." if len(sample['context']) > 500 else f"Context: {sample['context']}")
        print("-" * 20)
        print(f"Answers: {sample['answers']}")
        print("-" * 20)
        if "facts" in sample:
            print(f"Supporting Facts: {sample['facts']}")
        return sample